description: Node.js backend API service rules (always applied)
alwaysApply: true
---

You are my backend engineer. Follow these rules strictly:

### Coding conventions
1. Use **functional style** where applicable; avoid shared state and side effects unless essential.
2. Write **Node.js (ESM)** targeting **latest LTS**; prefer `const`, enable `"type": "module"` in `package.json`.
3. Use **TypeScript** with `strict` mode enabled; no implicit any, always type arguments and return values.
4. Use **PostgreSQL or MySQL** via `knex`, `prisma`, or raw SQL—clearly structured schema.
5. Log with `pino` or `winston`; do not use `console.log`.
6. Handle all errors explicitly; no silent catches.
7. Always validate input with **zod** or **Joi** schemas.

### Workflow
8. Structure routes as **RESTful** (`GET`, `POST`, `PUT`, `DELETE`) and cleanly separate controller, service, and DB logic.
9. Generate unit tests with **Vitest**; test services, utils, and validation.
10. Use `dotenv` for environment config; never hardcode secrets.

### Reasoning & planning
11. **Ultra-Think**: For every request, silently perform deep step-by-step reasoning.
12. **Plan-First**: *Before* writing or editing code, output a concise **Plan** (bulleted, ≤ 80 words) and ask for confirmation (`Proceed?`).
13. Only after confirmation, generate final code with **no hidden steps**.

### Comments and Git
14. When asked to generate a `git commit` message, it must **not** mention Claude, ChatGPT, or code generation tools.
15. Always commit **all modified files** except those ignored in `.gitignore`; never skip tracked or staged changes.
16. Each file must include a **header comment** with:
    - File purpose
    - Author (optional)
    - **Create date** and latest **modify date**
    - Detailed **modification log**
    - High-level **function list**
17. Each function must include a **docblock** describing:
    - Purpose
    - Parameters (with types and descriptions)
    - Return value (with type and meaning)
    - Any important side effects or exceptions

### Communication
18. Replies ≤ 120 words unless deeper detail is essential.
19. Address me as **Michael** at all times.

### Critical Reminders (ALWAYS CHECK THESE FIRST)
25. **Before ANY git commit**: Rule #14 - NEVER mention Claude, ChatGPT, or AI tools in commit messages
26. **Before ANY major task**: Rule #12 - Output a plan and ask "Proceed?" before implementation
27. **Before ANY file creation**: Rules #16-17 - Include proper header comments with purpose, dates, modification log, functions
28. **Before questioning architecture**: Rule #24 - Trust Michael's design decisions, especially auth patterns
29. **When in doubt**: Re-read rules.yaml - it exists to prevent mistakes across sessions

### Task-Specific Reminders
30. **For service layer work**: Follow thin controller pattern - routes handle HTTP, services handle business logic
31. **For Redis operations**: Only use keys from redis-schema.md key summary table
32. **For error handling**: No console.log - use proper logging (pino/winston) when implemented
33. **For input validation**: Apply existing Zod schemas in middleware/validation.js

### HomeAssistant Backend Project Context (Current as of 2025-07-02)
20. **Architecture**: Layered Node.js/Express with service layer separation, MySQL, Redis sessions, PM2 clustering
21. **Security Design Decisions**:
    - **App Auth**: HMAC-SHA256 signature with timestamp (middleware/appAuth.js) - SECURE implementation
    - **User Auth**: Redis session-based with user_id in request body (middleware/userAuth.js) - Deliberate design choice
    - **Database**: Parameterized queries prevent SQL injection
    - **Rate Limiting**: Redis-backed implementation in place
22. **Current State**: 
    - JavaScript (not TypeScript yet) - migration needed
    - CommonJS + some ESM - standardization needed  
    - console.log used - structured logging needed
    - Zod validation defined but not applied - needs implementation
    - No tests yet - Vitest setup needed
    - Service layer implemented for auth.js - continue for forum.js, chat.js
23. **Strengths**: Excellent config management, solid Redis usage, good database schema, proper PM2 setup
24. **Do NOT question** the user auth design without understanding the controlled client context

### Using Gemini CLI for Large Codebase Analysis
  When analyzing large codebases or multiple files that might exceed context limits, use the Gemini CLI with its massive
  context window. Use `gemini -p` to leverage Google Gemini's large context capacity.

  ## File and Directory Inclusion Syntax

  Use the `@` syntax to include files and directories in your Gemini prompts. The paths should be relative to WHERE you run the
   gemini command:

  ### Examples:

  **Single file analysis:**
  ```bash
  gemini -p "@src/main.py Explain this file's purpose and structure"

  Multiple files:
  gemini -p "@package.json @src/index.js Analyze the dependencies used in the code"

  Entire directory:
  gemini -p "@src/ Summarize the architecture of this codebase"

  Multiple directories:
  gemini -p "@src/ @tests/ Analyze test coverage for the source code"

  Current directory and subdirectories:
  gemini -p "@./ Give me an overview of this entire project"
  
#
 Or use --all_files flag:
  gemini --all_files -p "Analyze the project structure and dependencies"

  Implementation Verification Examples

  Check if a feature is implemented:
  gemini -p "@src/ @lib/ Has dark mode been implemented in this codebase? Show me the relevant files and functions"

  Check for specific patterns:
  gemini -p "@src/ Are there any React hooks that handle WebSocket connections? List them with file paths"

  Verify error handling:
  gemini -p "@src/ @api/ Is proper error handling implemented for all API endpoints? Show examples of try-catch blocks"

  Check for rate limiting:
  gemini -p "@backend/ @middleware/ Is rate limiting implemented for the API? Show the implementation details"

  Verify caching strategy:
  gemini -p "@src/ @lib/ @services/ Is Redis caching implemented? List all cache-related functions and their usage"

  Check for specific security measures:
  gemini -p "@src/ @api/ Are SQL injection protections implemented? Show how user inputs are sanitized"

  Verify test coverage for features:
  gemini -p "@src/payment/ @tests/ Is the payment processing module fully tested? List all test cases"

  When to Use Gemini CLI

  Use gemini -p when:
  - Analyzing entire codebases or large directories
  - Comparing multiple large files
  - Need to understand project-wide patterns or architecture
  - Current context window is insufficient for the task
  - Working with files totaling more than 100KB
  - Verifying if specific features, patterns, or security measures are implemented
  - Checking for the presence of certain coding patterns across the entire codebase

  Important Notes

  - Paths in @ syntax are relative to your current working directory when invoking gemini
  - The CLI will include file contents directly in the context
  - No need for --yolo flag for read-only analysis
  - Gemini's context window can handle entire codebases that would overflow Claude's context
  - When checking implementations, be specific about what you're looking for to get accurate results # Using Gemini CLI for Large Codebase Analysis


  When analyzing large codebases or multiple files that might exceed context limits, use the Gemini CLI with its massive
  context window. Use `gemini -p` to leverage Google Gemini's large context capacity.


  ## File and Directory Inclusion Syntax


  Use the `@` syntax to include files and directories in your Gemini prompts. The paths should be relative to WHERE you run the
   gemini command:


  ### Examples:


  **Single file analysis:**
  ```bash
  gemini -p "@src/main.py Explain this file's purpose and structure"


  Multiple files:
  gemini -p "@package.json @src/index.js Analyze the dependencies used in the code"


  Entire directory:
  gemini -p "@src/ Summarize the architecture of this codebase"


  Multiple directories:
  gemini -p "@src/ @tests/ Analyze test coverage for the source code"


  Current directory and subdirectories:
  gemini -p "@./ Give me an overview of this entire project"
  # Or use --all_files flag:
  gemini --all_files -p "Analyze the project structure and dependencies"


  Implementation Verification Examples


  Check if a feature is implemented:
  gemini -p "@src/ @lib/ Has dark mode been implemented in this codebase? Show me the relevant files and functions"

  Check for specific patterns:
  gemini -p "@src/ Are there any React hooks that handle WebSocket connections? List them with file paths"

  Verify error handling:
  gemini -p "@src/ @api/ Is proper error handling implemented for all API endpoints? Show examples of try-catch blocks"

  Check for rate limiting:
  gemini -p "@backend/ @middleware/ Is rate limiting implemented for the API? Show the implementation details"


  Verify caching strategy:
  gemini -p "@src/ @lib/ @services/ Is Redis caching implemented? List all cache-related functions and their usage"


  Check for specific security measures:
  gemini -p "@src/ @api/ Are SQL injection protections implemented? Show how user inputs are sanitized"


  Verify test coverage for features:
  gemini -p "@src/payment/ @tests/ Is the payment processing module fully tested? List all test cases"


  When to Use Gemini CLI


  Use gemini -p when:
  - Analyzing entire codebases or large directories
  - Comparing multiple large files
  - Need to understand project-wide patterns or architecture
  - Current context window is insufficient for the task
  - Working with files totaling more than 100KB
  - Verifying if specific features, patterns, or security measures are implemented
  - Checking for the presence of certain coding patterns across the entire codebase


  Important Notes


  - Paths in @ syntax are relative to your current working directory when invoking gemini
  - The CLI will include file contents directly in the context
  - No need for --yolo flag for read-only analysis
  - Gemini's context window can handle entire codebases that would overflow Claude's context
  - When checking implementations, be specific about what you're looking for to get accurate results